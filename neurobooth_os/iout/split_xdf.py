import os
import pyxdf
import pylsl
import liesl
import time
import numpy as np
import os.path as op
from datetime import datetime
from pathlib import Path
from typing import NamedTuple, Optional, Any, List
import h5io

from neurobooth_os.iout import metadator as meta
from neurobooth_terra import Table
import neurobooth_os.config as cfg

# This file keeps track of XDF files that have yet to be split
DONT_SPLIT_FILENAME = "split_tohdf5.csv"


def compute_clocks_diff() -> float:
    """
    Compute difference between local LSL and Unix clock.
    :returns: The offset between the clocks (in seconds).
    """
    return time.time() - pylsl.local_clock()


# TODO: Correct device metadata prior to write
def split_sens_files(
    fname: str,
    log_task_id: Optional[str] = None,
    task_id: Optional[str] = None,
    conn=None,
    folder: str = "",
    dont_split_xdf_fpath: Optional[str] = None,
) -> List[str]:
    """Split an XDF file into multiple HDF5 files (one per sensor).
    Also contains logic for logging expected files to the database.

    :param fname: Name of the XDF file to split.
    :param log_task_id: Task log ID for the database. Must be specified if logging to the database.
    :param task_id: Task ID for the database. Must be specified if only splitting relevant device IDs.
    :param conn: Connection to the database. Do not make database updates if not specified.
    :param folder: Path to the XDF file (i.e., /path/to/fname)
    :param dont_split_xdf_fpath: If provided, do not split the XDF file. Instead, add to the file at the given path to
        be split later. Database logging will still occur.
    :returns: The list of HDF5 files generated by the split.
    """

    t0 = time.time()
    xdf_path = os.path.join(folder, fname)
    device_ids = meta.get_device_ids(task_id)

    # Read XDF file
    device_data = _parse_xdf(xdf_path, device_ids)

    # Correct old data stream formats
    device_data = [_correct_hdf5(d) for d in device_data]

    # Either 1) Write the HDF5 files or 2) save note to do so later
    if dont_split_xdf_fpath is None:
        _write_device_hdf5(device_data)
    else:
        with open(os.path.join(dont_split_xdf_fpath, DONT_SPLIT_FILENAME), "a+") as f:
            f.write(f"{fname},{task_id}\n")

    # Log expected files to the database
    if conn is not None and log_task_id is not None:
        _log_to_database(device_data, conn, log_task_id)

    print(f"SPLIT XDF {task_id} took: {time.time() - t0}")
    return [d.hdf5_path for d in device_data]


class DeviceData(NamedTuple):
    device_id: str
    device_data: Any
    marker_data: Any
    video_files: Optional[str]
    sensor_ids: List[str]
    hdf5_path: str


def _parse_xdf(xdf_path: str, device_ids: Optional[List[str]] = None) -> List[DeviceData]:
    """
    Split an XDF file into device/stream-specific HDF5 files.

    :param xdf_path: The path to the XDF file to parse.
    :param device_ids: If provided, only parse files corresponding to the specified devices.
    :returns: A structured representation of information extracted from the XDF file for each device.
    """
    folder, file_name = os.path.split(xdf_path)
    data, _ = pyxdf.load_xdf(xdf_path, dejitter_timestamps=False)

    # Find marker stream to associate with each device
    marker = [d for d in data if d["info"]["name"] == ["Marker"]][0]

    # Get video file names for each device "videofiles" marker is present
    video_files = {}
    if "videofiles" in [d["info"]["name"][0] for d in data]:
        video_data = [v for v in data if v["info"]["name"] == ["videofiles"]]
        # Video file marker format is ["streamName, fname.mov"]
        for d in video_data[0]["time_series"]:
            if d[0] == "":
                continue
            stream_id, file_id = d[0].split(",")
            if folder:
                file_id = f"{folder}/{file_id}"
            if stream_id in video_files:
                video_files[stream_id] += f", {file_id}"
            else:
                video_files[stream_id] = file_id

    # Parse device data into more structured format; associate with marker and videos
    results = []
    for device_data in data:
        device_name = device_data["info"]["name"][0]

        # Exclude streams are associated with other devices and not represented by their own HDF5 file
        if device_name in ["Marker", "videofiles"]:
            continue

        device_id = device_data["info"]["desc"][0]["device_id"][0]
        sensors_ids = eval(device_data["info"]["desc"][0]["sensor_ids"][0])  # Dirty trick for converting into a list

        if (device_ids is not None) and (device_id not in device_ids):  # Only split specified devices
            continue

        results.append(DeviceData(
            device_id=device_id,
            device_data=device_data,
            marker_data=marker,
            video_files=video_files[device_name] if device_name in video_files else None,
            sensor_ids=sensors_ids,
            hdf5_path=_make_hdf5_path(xdf_path, device_id, sensors_ids),
        ))

    return results


def _make_hdf5_path(xdf_path: str, device_id: str, sensor_ids: List[str]) -> str:
    """
    Generate a path for a device HDF5 file extracted from an XDF file.

    :param xdf_path: Full path to the XDF file.
    :param device_id: ID string for the device.
    :param sensor_ids: List of ID strings for each included sensor.
    :returns: A standardized file name for corresponding device HDF5 file.
    """
    sensor_list = "-".join(sensor_ids)
    head, _ = op.splitext(xdf_path)
    return f"{head}-{device_id}-{sensor_list}.hdf5"


def _write_device_hdf5(device_data: List[DeviceData]) -> None:
    """
    Write the HDF5 files containing extracted device data.
    :param device_data: A list of objects containing the extracted device information.
    """
    for dev in device_data:
        data_to_write = {"marker": dev.marker_data, "device_data": dev.device_data}
        h5io.write_hdf5(dev.hdf5_path, data_to_write, overwrite=True)


LOG_SENSOR_COLUMNS = [
    "log_task_id",
    "true_temporal_resolution",
    "true_spatial_resolution",
    "file_start_time",
    "file_end_time",
    "device_id",
    "sensor_id",
    "sensor_file_path",
]


def _log_to_database(
        device_data: List[DeviceData],
        conn,
        log_task_id: str,
) -> None:
    """
    Log the names of sensor data files to the database.
    :param device_data: A list of objects containing the extracted device information.
    :param conn: A database connection object.
    :param log_task_id: The value to insert into the log_task_id column.
    """
    table_sens_log = Table("log_sensor_file", conn=conn)

    for dev in device_data:
        # Calculate timing characteristics of the data stream
        time_offset = compute_clocks_diff()
        timestamps = dev.device_data["time_stamps"]
        start_time = datetime.fromtimestamp(timestamps[0] + time_offset).strftime("%Y-%m-%d %H:%M:%S")
        end_time = datetime.fromtimestamp(timestamps[-1] + time_offset).strftime("%Y-%m-%d %H:%M:%S")
        temporal_resolution = 1 / np.median(np.diff(timestamps))

        # Construct the set of file names associated with the sensor
        if dev.video_files is None:
            sensor_file_paths = dev.hdf5_path
        else:
            sensor_file_paths = f"{dev.hdf5_path}, {dev.video_files}"
        sensor_file_paths = "{" + sensor_file_paths + "}"

        for sensor_id in dev.sensor_ids:
            vals = [(
                log_task_id,
                temporal_resolution,
                None,
                start_time,
                end_time,
                dev.device_id,
                sensor_id,
                sensor_file_paths,
            )]
            table_sens_log.insert_rows(vals, LOG_SENSOR_COLUMNS)


def _correct_hdf5(device_data: DeviceData) -> DeviceData:
    """
    Correct the format of old HDF5 data streams (in memory prior to writing).

    :param device_data: The device data stream to correct.
    :returns: The corrected device data stream.
    """
    # Import here to prevent circular import
    from neurobooth_os.iout.correct_hdf5 import HDF5_CORRECT_HOOKS, correct_marker

    # Correct the marker data stream
    device_data = correct_marker(device_data)

    # Correct the device data stream if a hook is registered for the device ID
    if device_data.device_id in HDF5_CORRECT_HOOKS:
        device_data = HDF5_CORRECT_HOOKS[device_data.device_id](device_data)

    return device_data


def get_xdf_name(session: liesl.Session, fname_prefix: str) -> str:
    """Get with most recent session XDF file name.

    :param session: The current liesl Session object. Or object with the session.folder attribute.
    :param fname_prefix: Prefix of the xdf file name.
    :returns: File name of the XDF file.
    """
    fname = session.folder / Path(fname_prefix + ".xdf")
    base_stem = fname.stem.split("_R")[0]
    count = 0
    for f in fname.parent.glob(fname.stem + "*.xdf"):
        base_stem, run_counter = f.stem.split("_R")
        count = max(int(run_counter), count)
    run_str = "_R{0:03d}".format(count)
    final_fname = str(fname.with_name(base_stem + run_str).with_suffix(".xdf"))
    return final_fname


def create_h5_from_csv(
        dont_split_xdf_fpath: str,
        conn,
        server_name: Optional[str] = None
) -> None:
    """
    :param dont_split_xdf_fpath: Path to the CSV file containing filenames and task IDs.
    :param conn: Connection to the database.
    :param server_name: A string matching one of the servers defined in the neurobooth_config.json file.
        If None, the name will be determined from the Windows User Profile, if possible.
    """
    if server_name is None:
        server_name = cfg.get_server_name_from_env()
        if server_name is None:
            raise Exception("A server name is required if the Windows user is not in (CTR, ACQ, or STM)")

    lines_todo = []
    fname = os.path.join(dont_split_xdf_fpath, DONT_SPLIT_FILENAME)
    import csv

    # Read file and split to HDF5 in the same directory
    with open(fname, newline="") as csvfile:
        for row in csv.reader(csvfile, delimiter=",", quotechar="|"):
            xdf_path, task_id = row[0], row[1]

            # Change to NAS path if necessary
            if not os.path.exists(row[0]):
                xdf_path = xdf_path.replace('\\', '/')
                xdf_path = xdf_path.replace(
                    cfg.neurobooth_config.server_by_name(server_name).local_data_dir[:-1],
                    cfg.neurobooth_config.remote_data_dir
                )

            out = split_sens_files(xdf_path, task_id=task_id, conn=conn)
            if len(out) == 0:
                lines_todo.append(row)

    # Rewrite file in case some XDF didn't get split
    with open(fname, "w") as f:
        for lns in lines_todo:
            f.write(f"{lns[0]},{lns[1]}")
